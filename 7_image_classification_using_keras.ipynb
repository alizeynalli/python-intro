{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consolidated-orlando",
   "metadata": {
    "id": "consolidated-orlando",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"input/FAU.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Image Classification with Keras\n",
    "    \n",
    "*David B. Blumenthal*, *Suryadipto Sarkar*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-audience",
   "metadata": {
    "id": "passing-audience",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## What is Tensorflow?\n",
    "\n",
    "\n",
    "Tensorflow is an open-source end-to-end platform that facilitates designing and deploying Machine Learning models using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_Xzz20cGx--p",
   "metadata": {
    "id": "_Xzz20cGx--p",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## What is Keras?\n",
    "\n",
    "\n",
    "Keras is an API built on top of TensorFlow, that supports deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "obvious-foundation",
   "metadata": {
    "id": "obvious-foundation",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# IMPORT REQUIRED LIBRARIES:\n",
    "# --------------------------\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from skimage.io import imread_collection\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RbG3DPw3ds44",
   "metadata": {
    "id": "RbG3DPw3ds44",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Note:**\n",
    "* This is the way to mount drive and read images directly from Google Drive. However, since we have 24,000 images, this will take a while. Therefore, I will show you just this first step on Spyder as I can access the files locally.\n",
    "* Then, we can just upload the numpy arrays here and work on those.\n",
    "\n",
    "  * Anyway, you only need to do this the very first time that you read in the data.\n",
    "  * .py script can be accessed over this link: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "g8tS9h5DcaOm",
   "metadata": {
    "id": "g8tS9h5DcaOm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IBdKo70mIJTe",
   "metadata": {
    "id": "IBdKo70mIJTe"
   },
   "source": [
    "## Classification Dataset\n",
    "\n",
    "\n",
    "We will make use of the popular 'Cats vs Dogs' classification dataset.\n",
    "\n",
    "Dataset download link: https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rL0UpsnS7fMq",
   "metadata": {
    "id": "rL0UpsnS7fMq"
   },
   "source": [
    "## Simple exercise:\n",
    "*(Uncomment the code for answer)*\n",
    "\n",
    "\n",
    "* Read all of the .jpeg and .png images from a folder.\n",
    "\n",
    "* Save all of the images in a single list (i.e., a list of image arrays). Note that this is the most common way of working with data in python. We often save all of the data into numpy lists or dataframes or similar structures.\n",
    "\n",
    "There are many ways of doing this. I have made use of **opencv** as it is one of the most popular Computer Vision libraries in python. Read more at: https://github.com/opencv/opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rNWcIWRR7eyt",
   "metadata": {
    "id": "rNWcIWRR7eyt"
   },
   "outputs": [],
   "source": [
    "# Simple way to read in all of the data:\n",
    "# --------------------------------------\n",
    "\n",
    "####################################################################################\n",
    "# CATS_folder='/Users/surya/Downloads/kagglecatsanddogs_3367a (1)/PetImages/Cat' # CATS_folder='Cat_folder_path' | # If reading from Drive: CATS_folder='/content/drive/MyDrive/PetImages/Cat'\n",
    "# DOGS_folder='/Users/surya/Downloads/kagglecatsanddogs_3367a (1)/PetImages/Dog' # DOGS_folder='Dog_folder_path' | # If reading from Drive: DOGS_folder='/content/drive/MyDrive/PetImages/Dog'\n",
    "\n",
    "# imdir = CATS_folder # or, DOGS_folder\n",
    "# ext = ['png', 'jpg'] # add other image formats for other datasets\n",
    "# files = []\n",
    "# [files.extend(glob.glob(imdir + '*.' + e)) for e in ext]\n",
    "# images = [cv2.imread(file) for file in files]\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ENlkxrVE-MRx",
   "metadata": {
    "id": "ENlkxrVE-MRx"
   },
   "source": [
    "## Data cleanup:\n",
    "\n",
    "In real-world imaging datasets, we often require a lot of preprocessing (especially when it comes to medical data-for example, radiological scans or cellular microscopy images).\n",
    "\n",
    "* In this tutorial, however, we will not focus too much into image pre-processing as this is not an advanced imaging course.\n",
    "\n",
    "* However, we will give an example of how to remove corrupted files in this particular dataset:\n",
    "\n",
    "\n",
    "```\n",
    "* I would advise you to take a look at the os library in python (https://docs.python.org/3/library/os.html).\n",
    "* It is very handy when you want to interact with the operating system using in-built python functions.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63TadhU1-M2c",
   "metadata": {
    "id": "63TadhU1-M2c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We will remove the corrupted images by making use of the following code (already provided in the official Keras page for the 'Dogs vs Cats' dataset handling):\n",
    "# --------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "num_skipped = 0\n",
    "for folder_name in (\"Cat\", \"Dog\"):\n",
    "    # folder_path = os.path.join(\"Parent_folder_path\", folder_name)\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            fobj = open(fpath, \"rb\")\n",
    "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "        finally:\n",
    "            fobj.close()\n",
    "\n",
    "        if not is_jfif:\n",
    "            num_skipped += 1\n",
    "            # Delete corrupted image\n",
    "            os.remove(fpath)\n",
    "\n",
    "print(\"Deleted %d images\" % num_skipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L_PLWCZrAhc-",
   "metadata": {
    "id": "L_PLWCZrAhc-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resize the images:\n",
    "\n",
    "We will reduce the size of each image. Why?\n",
    "\n",
    "* To save space, and for quick processing times.\n",
    "\n",
    "* To avoid overfitting (i.e., providing too much unnecessary information per image)\n",
    "\n",
    "\n",
    "```\n",
    "There are many ways of doing this. We will make use of the open-cv library.\n",
    "```\n",
    "**Note: It is a very common mistake to just reduce the images without preserving aspect ratio. Do not make this mistake. Be sure to use the correct ratio to reduce image size.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-e2oTkAjAgZV",
   "metadata": {
    "id": "-e2oTkAjAgZV",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "IMG_SIZE=32\n",
    "\n",
    "directory='/Users/surya/Downloads/kagglecatsanddogs_3367a (1)/PetImages'\n",
    "classes=['Cat', 'Dog']\n",
    "\n",
    "training_data = []\n",
    "\n",
    "def LoadImagesAndLabels():\n",
    "    for c in classes:\n",
    "        path=os.path.join(directory,c) # path to \"0\" or \"1\" directory\n",
    "        class_num=classes.index(c)\n",
    "    \n",
    "    \n",
    "        for filename in os.listdir(path):\n",
    "            fpath = os.path.join(path, filename)\n",
    "            fobj = open(fpath, \"rb\")\n",
    "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(100)\n",
    "            if is_jfif:\n",
    "                img = cv2.imread(os.path.join(path,filename))\n",
    "            if img is not None:\n",
    "                img=cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "                img=img.tolist()\n",
    "                training_data.append([img,class_num])\n",
    "    return training_data\n",
    "LoadImagesAndLabels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KlmRps9rCEW6",
   "metadata": {
    "id": "KlmRps9rCEW6"
   },
   "source": [
    "## Homework:\n",
    "\n",
    "1. Print the number of images in each class. And the total number of images. Verify that the total sample size equals the sum of samples in each of the individual classes.\n",
    "\n",
    "    ***Hint: Make use of len() or np.shape()[0] or df.shape[0]***\n",
    "\n",
    "\n",
    "2. Plot the first image from each of the two classes.\n",
    "\n",
    "    ***Hint: Read about the matplotlib library used for generating plots in python.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PFjyIH88-aKt",
   "metadata": {
    "id": "PFjyIH88-aKt"
   },
   "source": [
    "## Saving the data:\n",
    "\n",
    "* We often need to run ML algorithms many times with little tweaks in the model.\n",
    "\n",
    "* How do we achieve this? Of course, we could read the data every time and save it as a list as shown above.\n",
    "\n",
    "* However, this is a very time-consuming approach.\n",
    "\n",
    "* What is a better approach? Saving the data as a list of image arrays once, and save that for later use.\n",
    "\n",
    "\n",
    "\n",
    "Read about the **`.pickle`** and **`.npy`** file types. (Here, we will use pickle - don't forget to import the **** library.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jQhffyIxALJk",
   "metadata": {
    "id": "jQhffyIxALJk"
   },
   "outputs": [],
   "source": [
    "# # SYNTAX:\n",
    "# # -------\n",
    "# pickle_out=open(\"data.pickle\",\"wb\")\n",
    "# pickle.dump(data,pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paF8xtO2qnzt",
   "metadata": {
    "id": "paF8xtO2qnzt"
   },
   "source": [
    "## But first let's randomize the data before saving:\n",
    "\n",
    "* First option is using the random.shuffle() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ygq1hfo0rn04",
   "metadata": {
    "id": "Ygq1hfo0rn04"
   },
   "outputs": [],
   "source": [
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15O1KIQgromA",
   "metadata": {
    "id": "15O1KIQgromA"
   },
   "source": [
    "* Second option (exercise): Implement a function that does this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C0BZgUu0rZ3t",
   "metadata": {
    "id": "C0BZgUu0rZ3t"
   },
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "def Shuffle(X, y):\n",
    "    # * ENTER CODE HERE * #\n",
    "X, y=Shuffle(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6kCr1KA5r9FV",
   "metadata": {
    "id": "6kCr1KA5r9FV"
   },
   "source": [
    "* Solution (uncomment to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DI9cTKCor_qh",
   "metadata": {
    "id": "DI9cTKCor_qh"
   },
   "outputs": [],
   "source": [
    "# def Shuffle(X, y):\n",
    "    # X_shuffled=[]\n",
    "    # y_shuffled=[]\n",
    "    # length=len(y)\n",
    "    # index=list(range(length))\n",
    "    # random.Random(12).shuffle(index)\n",
    "    # for i in range(length):\n",
    "        # X_shuffled.append(X[index[i]])\n",
    "        # y_shuffled.append(y[index[i]])\n",
    "    # return X_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E89SLfXNsT3F",
   "metadata": {
    "id": "E89SLfXNsT3F"
   },
   "source": [
    "## Finally, save shuffled data as .pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZJlCgHSUsevC",
   "metadata": {
    "id": "ZJlCgHSUsevC"
   },
   "outputs": [],
   "source": [
    "# Save the training data\n",
    "pickle_out=open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X,pickle_out)\n",
    "pickle_out.close()\n",
    "# Save the training labels\n",
    "pickle_out=open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sTCAVBx2tBoP",
   "metadata": {
    "id": "sTCAVBx2tBoP"
   },
   "source": [
    "# **CONVOLUTIONAL NEURAL NETWORKS (CNNs):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kwWqDKSNtx2T",
   "metadata": {
    "id": "kwWqDKSNtx2T",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why CNNs?\n",
    "\n",
    "* Convert data to embeddings/ features\n",
    "* Example: Converting images from pixel space to feature space\n",
    "* Enhances learning, reduces dimensionality, represents the data better\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1imb0ZgzQ6sK02SniXUf4VebiLL7A14xa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0sm_svbftR93",
   "metadata": {
    "id": "0sm_svbftR93"
   },
   "source": [
    "# Components/ Layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ULjcZ605a0b",
   "metadata": {
    "id": "9ULjcZ605a0b"
   },
   "source": [
    "### **Question**: Looking at the kernel matrix provided above, what kind of pattern do you think it is meant to detect?\n",
    "\n",
    "  * Answer: Vertical edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cJs1ePysMW9R",
   "metadata": {
    "id": "cJs1ePysMW9R"
   },
   "source": [
    "## I. Convolutional Layer:\n",
    "* Helps extract local patterns in the data (here, image).\n",
    "* Also helps reduce the number of features. But that is a byproduct of the convolution operation, it is not the main objective. The main objective is to extract meaningful local patterns.\n",
    "+ ### **Note: If the image is an RGB (3-channel image), the convolved image is also 3-channel. If the input image is a gray (1-channel) image, the convolved image is also single-channeled.**\n",
    "  + This is because the kernel is applied on each channel separately for convolution.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## **An oversimplified example of convolution:**\n",
    "(Note: Kernel size 4*4, padding 0, stride length=1)\n",
    "![](https://drive.google.com/uc?export=view&id=1QngPcDB6pwkIz5Yhftr-3Rj8WEgD5h0Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tS9egfG_NeaL",
   "metadata": {
    "id": "tS9egfG_NeaL"
   },
   "source": [
    "## II. Pooling Layer:\n",
    "\n",
    "* The main function of the pooling layer is to reduce dimensionality.\n",
    "* Two popular types of pooling: MaxPooling, and AveragePooling.\n",
    "* AveragePooling also helps in noise reduction.\n",
    "\n",
    "<br>\n",
    "\n",
    "## **A simple example of Pooling:**\n",
    "(Note: Pooling window size 3*3)\n",
    "![](https://drive.google.com/uc?export=view&id=17FzHhVdWnI0nrQDkOKiw-JlLqt0mJ9_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ftwhH-7wxcl4",
   "metadata": {
    "id": "ftwhH-7wxcl4"
   },
   "source": [
    "## III. Feedforward Layer:\n",
    "* Standard Neural Network architecture used for classification\n",
    "\n",
    "<br>\n",
    "\n",
    "## **A simple fully-connected, feed forward neural network:**\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1bdz0dUNBzt7Ovo5m57-U_-qiBsq846sE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xmt9MfVF-25i",
   "metadata": {
    "id": "xmt9MfVF-25i"
   },
   "source": [
    "## Exercise:\n",
    "* Multi-class classification requires a Softmax layer, but two-class classification does not. Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XMUUcT1dC9wn",
   "metadata": {
    "id": "XMUUcT1dC9wn"
   },
   "source": [
    "# Note on the 'Flatten' layer:\n",
    "* This is really not a layer in the conventional sense, although it is defined in the tensorflow.keras.layers.\n",
    "* This is a function used to convert the features (or weights) after pooling, to be fed into the aforementioned feedforward neural network for classification.\n",
    "\n",
    "  * We will see this a little later when we design the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-7FIWET6D4uc",
   "metadata": {
    "id": "-7FIWET6D4uc"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gNCI6loacVDY",
   "metadata": {
    "id": "gNCI6loacVDY"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25EWs3eUCoQE",
   "metadata": {
    "id": "25EWs3eUCoQE"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ECnJr2e80Vva",
   "metadata": {
    "id": "ECnJr2e80Vva"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbV7RxjMxt2h",
   "metadata": {
    "id": "fbV7RxjMxt2h"
   },
   "source": [
    "# **Homework reading:**\n",
    "\n",
    "### Basic ideas behind machine learning and AI:\n",
    "* What do we mean by 'learning' and 'intelligent' systems?\n",
    "* What are the three main types of machine learning, and what are the differences?\n",
    "\n",
    "### Artificial neural networks:\n",
    "* NN-related terms: Neurons, layers, activation functions, fully connected networks, multi-layer perceptron\n",
    "* Hyperparameter tuning and model improvement basics: Learning rate, no. of neurons oer layer, how to set model size (i.e., no. and type of layers)\n",
    "* Learning-related: Learning rate, backpropagation, gradient descent\n",
    "\n",
    "### Optional reading (slightly more advanced):\n",
    "* What is transfer learning? What are pre-trained models, and how to use them? Why pre-trained models?\n",
    "* What is overfitting? How to 1. detect 2. tackle overfitting?\n",
    "  * Regularization, Dropout, resampling, oversampling (read about 'SMOTE') and undersampling, data augmentation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SV56FzfOiqRX",
   "metadata": {
    "id": "SV56FzfOiqRX"
   },
   "source": [
    "# **Designing the model:**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### A schematic representation of our model:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1bDTsN4kiXvC1gh2vlPeVtbAhgmiu_dBn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EmnpyJz7i8HT",
   "metadata": {
    "id": "EmnpyJz7i8HT"
   },
   "outputs": [],
   "source": [
    "X=pickle.load(open(\"X.pickle\",\"rb\"))\n",
    "y=pickle.load(open(\"y.pickle\",\"rb\"))\n",
    "\n",
    "# Normalize data\n",
    "X=np.asarray(X)/255.0\n",
    "# X=X.tolist()\n",
    "\n",
    "y = np.array(y)\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(  Conv2D(64,(3,3),input_shape=X.shape[1:])  )\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(  Conv2D(32,(2,2))    )\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.add(Dense(64))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# history=model.fit(X, y, batch_size=32, shuffle=True, sample_weight=None, epochs=50,validation_split=0.1, verbose = 1, callbacks=[callback]) # seed=100,         \n",
    "\n",
    "history=model.fit(X, y, batch_size=32, shuffle=True, sample_weight=None, epochs=50,validation_split=0.1, verbose = 1) # seed=100,         \n",
    "\n",
    "\n",
    "# model.fit(X,y,batch_size=32,epochs=25,validation_split=0.1)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "khzNz3mCi7Ul",
   "metadata": {
    "id": "khzNz3mCi7Ul"
   },
   "source": [
    "## Follow-up exercise:\n",
    "* How many neurons are there in each of the aforementioned layers?\n",
    "  * Answer: Conv-1: 64, Conv2: 32, Dense-1: 128, Dense-2: 1\n",
    "* Why have we not used a Softmax layer?\n",
    "  * Answer: Because in 2-class classification, the probabilities will anyway add up to 1.\n",
    "* Run the model by inverting the no. of neurons in the first two Conv2D layers. How does the model perform (better/ worse)?\n",
    "* How does the model perform if we make use of stochastic gradient descent optimizer instead of Adam?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ip2dZppyxvdR",
   "metadata": {
    "id": "Ip2dZppyxvdR"
   },
   "source": [
    "### References:\n",
    "[1] Image generated with BioRender.com"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "7_image_classification_using_keras.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "interpreter": {
   "hash": "eef83daaa25a6a574d0d0479cc43f87e19b645fd2c92685619488cac1bd491c0"
  },
  "kernelspec": {
   "display_name": "python-intro",
   "language": "python",
   "name": "python-intro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
